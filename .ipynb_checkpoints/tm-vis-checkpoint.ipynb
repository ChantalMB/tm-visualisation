{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling and Interactive Visuals in R\n",
    "\n",
    "The [*Catalogue of Political and Personal Satires*](https://www.britishmuseum.org/collection/term/BIB294) is the the primary reference work for the study of British satirical prints of the 18th and 19th century hosted by the Department of Prints and Drawings in the British Museum-- it features things like [\"Mr Justic Bull's Decision in the case of Genuine Tea versus anti Genuine\"](https://www.britishmuseum.org/collection/object/P_1868-0808-8405):\n",
    "\n",
    "![Mr Justic Bull's Decision in the case of Genuine Tea versus anti Genuine](1274772001.jpg)\n",
    "\n",
    "In 2018, there was a project by the [Sussex Humanities Lab](https://www.sussex.ac.uk/research/centres/sussex-humanities-lab/) titled [*Curatorial Voice: legacy descriptions of art objects and their contemporary uses*](https://curatorialvoice.github.io/). It looked at the metadata of the BM Satire catalogue and focused on using methods to analyze the 1.5 million words written by the historian [Mary Dorothy George](https://en.wikipedia.org/wiki/M._Dorothy_George) between 1935 and 1954 to describe 12,552 Georgian satirical prints.\n",
    "\n",
    "For this tutorial, we will be using some of the [data collected](https://github.com/CuratorialVoice/code/tree/v1.0) during this project, and rather than looking specifically at the descriptions of Mary Dorothy George, we'll look at descriptions from the BM Satire catalogue of prints which were published during the early modern period (1500-1800).\n",
    "\n",
    "[Here's a video explaining topic modelling](https://youtu.be/gN2x_KjJI1o) by Prof. Graham. Essentially, topic modelling is the process of identifying themes or \"topics\" in a set of documents by deconstructing the text into however many topics you specify, and creating what the topics actually are based on how words relate to each other (if you want to learn more, [see here](http://www.scottbot.net/HIAL/index.html@p=19113.html)). LDA topic modelling is most commonly used for analyzing large text corpuses, but by applying it to the metadata of items in the BM Satire collection, we can look at both the language used to describe the items within the collection by the curators, as well as arrange the topics found in the collection across the early modern period based on their proportion and original publication date.\n",
    "\n",
    "**How this data has already been cleaned**:\n",
    "- Converted to CSV\n",
    "- Removed documents outside of 1500-1800\n",
    "- Removed formatters\n",
    "- Removed duplicates (marked by █)\n",
    "- Made double quotes within text singular\n",
    "\n",
    "(Make a habit of documenting how you cleaned your data, it allows those using your notebook to evaluate the change you've made and determine how these changes may affect the data.)\n",
    "\n",
    "\n",
    "## Creating the Topic Models\n",
    "\n",
    "Since this is a tutorial and we know the packages we'll be using, let's start with declaring those then load in our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning things up\n",
    "library(tidyverse)\n",
    "library(tidytext)\n",
    "\n",
    "# a package for stopwards that goes beyond what tidyverse provides\n",
    "library(stopwords)\n",
    "\n",
    "# for creating topic models\n",
    "library(reshape2)\n",
    "library(topicmodels)\n",
    "\n",
    "# palettes \n",
    "library(RColorBrewer)\n",
    "library(pals)\n",
    "\n",
    "# graphing and making it interactive\n",
    "library(ggplot2)\n",
    "library(ggiraph)\n",
    "\n",
    "# making streamgraphs!\n",
    "library(streamgraph)\n",
    "\n",
    "# for saving the graph\n",
    "library(htmlwidgets)\n",
    "library(IRdisplay)\n",
    "\n",
    "# read in data\n",
    "bmSatire <- read_csv(\"data/BMSatire-EM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now that we have all that, we can place our CSV data into a data frame and break it down into individual words. By placing it all in a `tibble`, we can ensure the data in each column is uniformly \"typed\" (text is all text, years are all numbers) so there's no unexpected behaviours/errors down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In our tibble \"bmSatire_df\":\n",
    "# the column \"id\" will correspond to the id in the CSV\n",
    "# and we are stripping the text column of numbers so it is uniformly \"text\"\n",
    "\n",
    "bmSatire_df <- tibble(id = bmSatire$id, \n",
    "                text = (str_remove_all(bmSatire$desc, \"[0-9]\")), \n",
    "                date = bmSatire$publicationDate)\n",
    "\n",
    "# check out what it looks like now!\n",
    "print(\"Showing: bmSatire_df\")\n",
    "bmSatire_df\n",
    "\n",
    "tidy_bmSatire <- bmSatire_df %>%\n",
    "  unnest_tokens(word, text)\n",
    "\n",
    "# all of the words from the document descriptions\n",
    "print(\"tidy_bmSatire\")\n",
    "tidy_bmSatire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even just from scrolling through the table of words generated through tokenization, you can get an idea of what these documents might about, perhaps what topics might appear. Of course, all of those \"a\" and \"of\" words are obscuring the view! So, it's time to remove the stop words.\n",
    "\n",
    "**Note**: There is both Spanish and French included in the descriptions of these documents, so we will be combining some multilingual stopword lists from [Quanteda](https://github.com/quanteda/stopwords) with the English set from Tidytext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Spanish and French stopwards\n",
    "langSet <- c(stopwords(language = \"es\", source = \"nltk\"),\n",
    "                  stopwords(language = \"fr\", source = \"nltk\"))\n",
    "\n",
    "# format them to match our data aka put them in a tibble \n",
    "my_stopwords <- tibble(word = langSet, lexicon = \"nltk\")\n",
    "\n",
    "# combine with tidytext stop_words\n",
    "stopwords <- bind_rows(stop_words, my_stopwords)\n",
    "\n",
    "# delete stopwords from our data\n",
    "tidy_bmSatire <- tidy_bmSatire %>%\n",
    "  anti_join(stopwords)\n",
    "\n",
    "# look at tidy_bmSatire now!\n",
    "tidy_bmSatire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the stop words are gone, does anything catch your eye? When looking at your own data, this step is an excellent place to pause and explore as it may reveal unique details that can put the work you're studying into context. For example, in this dataset, you may notice the token \"ca\"-- I initally thought it was an error neither I nor Tidytext caught, but when investigating further by returning to the original dataset and searching \"ca\", I discovered this was in reference to the French revolutionary war song, [\"Ça Ira\"](https://en.wikipedia.org/wiki/%C3%87a_Ira).\n",
    "\n",
    "Now we can start prep for topic modelling by first counting the word occurences, and then placing this count into a [document term matrix](https://bookdown.org/Maxine/tidy-text-mining/tidying-a-document-term-matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line will take a couple of seconds to finish!\n",
    "bmSatire_words <- tidy_bmSatire %>%\n",
    "  count(id, word, sort = TRUE)\n",
    "\n",
    "# take a look at the frequencies... lots of \"eye\" mentions at the top, why?\n",
    "bmSatire_words\n",
    "\n",
    "# turn that into a matrix\n",
    "# dtm = document term matrix\n",
    "dtm <- bmSatire_words %>%\n",
    "  cast_dtm(id, word, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now... the moment you've all been waiting for... topic modelling! Let's look at it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how many topics we're going to generate\n",
    "K <- 15\n",
    "\n",
    "# topic models need a random number for calculating the topics \n",
    "# setting the \"seed\"/based number for this allows those running this code to generate the same sequence of random numbers every time these calculations are done\n",
    "# so we'll always have the same results!\n",
    "set.seed(9161)\n",
    "\n",
    "# here's a solid, simple explanation of what LDA modelling is: http://bit.do/ELI5-LDA\n",
    "# compute the LDA model, inference via 1000 iterations of Gibbs sampling\n",
    "# this will also take a bit of time to run\n",
    "topicModel <- LDA(dtm, K, method=\"Gibbs\", control=list(iter = 1000, verbose = 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look a some of the results (posterior distributions)\n",
    "tmResult <- posterior(topicModel)\n",
    "\n",
    "# format of the resulting object\n",
    "attributes(tmResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of columns is the lengthOfVocab\n",
    "ncol(dtm)\n",
    "\n",
    "# topics are probability distributions over the entire lengthOfVocab\n",
    "beta <- tmResult$terms   # get beta from results\n",
    "dim(beta)                # K distributions over ncol(DTM) terms\n",
    "\n",
    "# the number of rows is size of collection\n",
    "nrow(dtm)\n",
    "\n",
    "# for every item in the dataset we have a probability distribution of its contained topics\n",
    "theta <- tmResult$topics \n",
    "dim(theta)                # number of documents distributed over K topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the way this works means that every topic we generated has every word, but in different proportions\n",
    "# so let's pull the top 7 terms from every topic to see how the proportions were distributed and thus\n",
    "# figure out what each topic is about\n",
    "top7termsPerTopic <- terms(topicModel, 7)\n",
    "topicNames <- apply(top7termsPerTopic, 2, paste, collapse=\" \")\n",
    "\n",
    "# look at your topics:\n",
    "for (t in topicNames) {\n",
    "    print(t)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you find that the model captures too much (or not enough) variability, try changing up the number of topics generated or the number of iterations in the LDA. \n",
    "\n",
    "Now that we have these topics generated, let's put the data into some form of context. You can learn from it just by looking at the topics, but how do the topic proportions change over time?\n",
    "\n",
    "## Making a `ggplot2` Graph Interactive\n",
    "\n",
    "First things first, we need to get our topics and their proportions associated with the date range from the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is creating groupings of decades from the original list of dates\n",
    "# using substr() we grab the first 3 numbers of the listed year, then attach a \"0\" to the end of that\n",
    "# ex. from 1743, substr() takes just \"174\" then pastes a \"0\", so it becomes 1740\n",
    "bmSatire$decade <- paste0(substr(bmSatire$publicationDate, 0, 3), \"0\")\n",
    "\n",
    "# we get the mean topic proportions per decade by calculating the\n",
    "# number of documents distributed over K topics (theta) by the list of decade\n",
    "topic_proportion_per_decade <- aggregate(theta, by = list(decade = bmSatire$decade), mean)\n",
    "\n",
    "# set topic names to aggregated columns\n",
    "colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames\n",
    "\n",
    "# reshape the data frame created for visualization\n",
    "vizDataFrame <- melt(topic_proportion_per_decade, id.vars = \"decade\")\n",
    "\n",
    "# make sure nothing looks fishy:\n",
    "vizDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All your data looks good? Okay! Here's how to place it in a simple stacked barplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a custom colour palette!\n",
    "# the brewer.pal() asks for the number of colours in the palette you're using, then the name of the palette from the ColorBrewer palette library\n",
    "# the argument after, \"(15)\", is how many colours we need for this graph\n",
    "mycolors <- colorRampPalette(brewer.pal(11, \"Spectral\"))(15)\n",
    "\n",
    "options(repr.plot.width = 17, repr.plot.height = 11)\n",
    "\n",
    "# a stacked bar plot of topic proportions per deacde\n",
    "ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) +\n",
    "    geom_bar(stat = \"identity\") +   # we want the heights of the bars to represent values in the data, so we set stat to \"identity\"\n",
    "    ylab(\"proportion\") + # then label the y axis\n",
    "    scale_fill_manual(values = mycolors) + # add our palette\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=17)) # this angles the labels (the dates) that are on the x-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, that's a pretty nice looking graph! It's clearly labelled and has a legend that you can reference to know what colour represents which topic. But wouldn't it be nice if, instead of having to reference a legend to understand what you're looking at, you could just mouse over each bar to see what they mean? This could make looking at the topic proportions a lot easier, so let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is to make a \"tooltip\", a box that appears will appear when you hover over a bar\n",
    "# we will have it show the topic name and the specific proportion \n",
    "# '<br/>' is the html line break-- it places the topic and proportion on separate lines for easy reading\n",
    "ttText <- paste(vizDataFrame$variable, \"<br/>\", vizDataFrame$value)\n",
    "\n",
    "# put our graph into a variable\n",
    "gg_bar <- ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) +\n",
    "    # we change the \"geom_bar\" (the part that makes the bars) to geom_bar_interactive \n",
    "    # I removed the legend for this example, but you can keep it there if you want \n",
    "    geom_bar_interactive(stat = \"identity\", show.legend = FALSE, tooltip = ttText, data_id = ttText) + \n",
    "    ylab(\"proportion\") +\n",
    "    scale_fill_manual(values = mycolors) +\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n",
    "\n",
    "# optional: make the tooltip a bit more aesthetically pleasing with some CSS styling\n",
    "tooltip_css <- \"background-color:gray;color:white;font-style:italic;padding:10px;border-radius:5px;font-family:Arial;\"\n",
    "\n",
    "stackedGph <- girafe(ggobj = gg_bar) %>%\n",
    "    # this pipe is optional, but can make exactly what you're looking more clear\n",
    "    # try removing it and compare the plots generated (don't forget to actually remove the pipe, \"%>%\", too!)\n",
    "    girafe_options(\n",
    "      opts_hover_inv(css = \"opacity:0.5;\"),\n",
    "      opts_hover(css = \"fill:clear;stroke:black;\"),\n",
    "      opts_tooltip(css = tooltip_css)\n",
    "    )\n",
    "\n",
    "# to save an interactive plot we need to export them as an HTML widget\n",
    "# we tell saveWidget that we want to save 'strgph'\n",
    "# and it saves in our working directory\n",
    "saveWidget(stackedGph, 'stackedGph.html')\n",
    "\n",
    "# to include this visual on a webpage or in your markdown documents, paste what is between the parenthesis where you want the visual in your document!\n",
    "# aka use iframes: https://www.w3schools.com/html/html_iframe.asp\n",
    "display_html('<iframe src=\"stackedGph.html\" width=\"900px\" height=\"770px\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done with ANY plot made using the `ggplot2` package! You can add `_interactive` to almost all forms of geometry like `geom_bar`, and create a plot with tooltips and other forms of interactivity that can make it more engaging for those exploring you visualisation. Check out the [`ggiraph` documentation](https://davidgohel.github.io/ggiraph/index.html) for more ways you can interact with your visuals in R!\n",
    "\n",
    "Overall, this visual is now very easy to explore and clearly shows how the topic proportions are broken down over each decade; however, some topics are visually broken up because of how the graph is separated into bars, making it more difficult to follow how each topic changes across the Early Modern period. How can we show this same data, but with more flow?\n",
    "\n",
    "## The `streamgraph` library\n",
    "\n",
    "The answer is streamgraphs! For a detailed overview of streamgraphs, check out this article by [Andy Kirk](https://www.visualisingdata.com/2010/08/making-sense-of-streamgraphs/), but to summarize, streamgraphs are a form of visualisation meant to show the \"flow\" of topics over a period of time. One of the features that makes this visualisation unique is that there is **no y-axis**. The verticality of the graph (so, whether the topic is on the top, in the middle, or on the bottom) is NOT significant, only the proportions are, and these proportions \"ebb and flow\" over time in relation to eachother.\n",
    "\n",
    "Let's take our data and create a streamgraph so you can better understand what you just read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strgph <- streamgraph(vizDataFrame, key=\"variable\", value=\"value\", date=\"decade\",\n",
    "                  width=\"920px\", height=\"350px\") %>%\n",
    "                  sg_fill_manual(values = mycolors) %>%\n",
    "                  sg_axis_y(0) %>%   # make sure to hide the y-axis so significance is not misattributed\n",
    "                  sg_legend(TRUE, \"Topic: \")\n",
    "\n",
    "# check out your streamgraph!\n",
    "saveWidget(strgph, 'strgph.html')\n",
    "display_html('<iframe src=\"strgph.html\" width=\"991\" height=\"450px\"></iframe>') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice and simple to plot thanks to the [dedicated streamgraph package](https://github.com/hrbrmstr/streamgraph) for R! As you can see, this graph resembles a timeline, but instead of plotting events, it shows each topic's proportion in relation to one another overtime, and adds the connectivity that the stacked bar graph lacks. If you want to look at one specific topic without having to select it manually by hovering, you can select it from the dropdown and it will highlight by itself!\n",
    "\n",
    "Time to explore! What insight can you gain on this data from looking at these visualisations? What could be the significance of that large spike around 1570? Do you notice any language in these descriptions that seems to be persistent? You can record any observations you make in your weekly notes.\n",
    "\n",
    "Now that you've learned all that, why not download this notebook on to your own machine and try running your own data through this code to see what happens? You can even go further than that and download the full R script for this notebook [here](https://github.com/ChantalMB/SaPP-workbook/blob/master/notebooks/topic-models/tm-sg.R) to open in RStudio and experiment with-- play with how its visualized, or WHAT is visualized.\n",
    "\n",
    "Here's some resources to get you started:\n",
    "- [R Graph Gallery](https://www.r-graph-gallery.com/index.html) for more ways to visualize data\n",
    "- [Tidytext](https://www.tidytextmining.com/) for cleaning your data\n",
    "- [Colour palettes in R](https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
